{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ec9fff1-5b49-4e93-8e28-0e70f4b98060",
   "metadata": {},
   "source": [
    "# Multimodal Article Question Answering Assistant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10ee99e2-92f4-46db-a89d-ea962d0a54a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from utility_functions import *\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from torch import load, matmul, argsort\n",
    "from torch.nn.functional import softmax\n",
    "import gradio as gr\n",
    "from IPython.display import Image\n",
    "\n",
    "import ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35610181-eb5a-45ea-bc98-f7077651c192",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80cc0142-e1fc-4235-b511-07752d300180",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load article contents\n",
    "text_content_list = load_from_json('data/text_content.json')\n",
    "image_content_list = load_from_json('data/image_content.json')\n",
    "\n",
    "# load embeddings\n",
    "text_embeddings = load('data/text_embeddings.pt', weights_only=True)\n",
    "image_embeddings = load('data/image_embeddings.pt', weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "15b1b808-4118-4ee7-bdc2-35e7d34f5f87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([86, 512])\n",
      "torch.Size([17, 512])\n"
     ]
    }
   ],
   "source": [
    "print(text_embeddings.shape)\n",
    "print(image_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4920cb4b-e0be-49b4-a4e8-e8ff87e84b4e",
   "metadata": {},
   "source": [
    "### Multimodal search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de72fca7-9478-4e44-97cd-24f3e9d1ca24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity_search(query_embed, target_embeddings, content_list, k=5, threshold=0.05, temperature=0.5):\n",
    "    \"\"\"\n",
    "    Perform similarity search over embeddings and return top k results.\n",
    "    \n",
    "    Args:\n",
    "        query_embed (torch.Tensor): Query embedding\n",
    "        target_embeddings (torch.Tensor): Target embeddings matrix to search over\n",
    "        content_list (list): List of content items corresponding to embeddings\n",
    "        k (int, optional): Number of top results to return. Defaults to 5.\n",
    "        threshold (float, optional): Minimum similarity score threshold. Defaults to 0.1.\n",
    "        temperature (float, optional): Temperature for softmax scaling. Defaults to 0.5.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (results, scores) where:\n",
    "            - results: List of top k content matches\n",
    "            - scores: Corresponding similarity scores\n",
    "    \"\"\"\n",
    "    # Calculate similarities\n",
    "    similarities = torch.matmul(query_embed, target_embeddings.T)\n",
    "    \n",
    "    # Rescale similarities via softmax\n",
    "    scores = torch.nn.functional.softmax(similarities/temperature, dim=1)\n",
    "    \n",
    "    # Get sorted indices and scores\n",
    "    sorted_indices = scores.argsort(descending=True)[0]\n",
    "    sorted_scores = scores[0][sorted_indices]\n",
    "    \n",
    "    # Filter by threshold and get top k\n",
    "    filtered_indices = [\n",
    "        idx.item() for idx, score in zip(sorted_indices, sorted_scores) \n",
    "        if score.item() >= threshold\n",
    "    ][:k]\n",
    "    \n",
    "    # Get corresponding content items and scores\n",
    "    top_results = [content_list[i] for i in filtered_indices]\n",
    "    result_scores = [scores[0][i].item() for i in filtered_indices]\n",
    "    \n",
    "    return top_results, result_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c20bc30-7448-4c2b-8cd8-3916aa0f4077",
   "metadata": {},
   "source": [
    "### Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "291b7472-7a41-4040-a1bf-618072e326b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def context_retrieval(query, text_embeddings, image_embeddings, text_content_list, image_content_list, \n",
    "                    text_k=15, image_k=5, \n",
    "                    text_threshold=0.01, image_threshold=0.25,\n",
    "                    text_temperature=0.25, image_temperature=0.5):\n",
    "    \"\"\"\n",
    "    Perform context retrieval over embeddings and return top k results.\n",
    "    \"\"\"\n",
    "    # embed query using CLIP\n",
    "    query_embed = embed_text(query)\n",
    "\n",
    "    # perform similarity search\n",
    "    text_results, _ = similarity_search(query_embed, text_embeddings, text_content_list, k=text_k, threshold=text_threshold, temperature=text_temperature)\n",
    "    image_results, _ = similarity_search(query_embed, image_embeddings, image_content_list, k=image_k, threshold=image_threshold, temperature=image_temperature)\n",
    "\n",
    "    return text_results, image_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c6df328-7f5c-4aa7-b2b2-2005b7bbf976",
   "metadata": {},
   "source": [
    "### Prompt Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d496214d-bf46-4859-9e48-aaf94d03328d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def construct_prompt(query, text_results, image_results):\n",
    "    \"\"\"\n",
    "    Construct a prompt for the LLM to generate a response.\n",
    "    \"\"\"\n",
    "\n",
    "    text_context = \"\"\n",
    "    for text in text_results:\n",
    "        if text_results:\n",
    "            text_context = text_context + \"**Article title:** \" + text['article_title'] + \"\\n\"\n",
    "            text_context = text_context + \"**Section:**  \" + text['section'] + \"\\n\"\n",
    "            text_context = text_context + \"**Snippet:** \" + text['text'] + \"\\n\\n\"\n",
    "\n",
    "    image_context = \"\"\n",
    "    for image in image_results:\n",
    "        if image_results:\n",
    "            image_context = image_context + \"**Article title:** \" + image['article_title'] + \"\\n\"\n",
    "            image_context = image_context + \"**Section:**  \" + image['section'] + \"\\n\"\n",
    "            image_context = image_context + \"**Image Path:**  \" + image['image_path'] + \"\\n\"\n",
    "            image_context = image_context + \"**Image Caption:** \" + image['caption'] + \"\\n\\n\"\n",
    "\n",
    "    # construct prompt\n",
    "    return f\"\"\"Given the query \"{query}\" and the following relevant snippets:\n",
    "\n",
    "    {text_context}\n",
    "    {image_context}\n",
    "\n",
    "    Please provide a concise and accurate answer to the query, incorporating relevant information from the provided snippets where possible.\n",
    "\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c9b10d5-bc0f-49f3-8a6b-f81be4ffa78d",
   "metadata": {},
   "source": [
    "### Chat UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5c3ed795-7430-464e-8d18-503e252ae0d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ProgressResponse(status='success', completed=None, total=None, digest=None)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ollama.pull('llama3.2-vision')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9271caf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to interact with the Ollama model\n",
    "def stream_chat(message, history):\n",
    "    \"\"\"\n",
    "    Streams the response from the Ollama model and sends it to the Gradio UI.\n",
    "    \n",
    "    Args:\n",
    "        message (str): The user input message.\n",
    "        history (list): A list of previous conversation messages.\n",
    "        \n",
    "    Yields:\n",
    "        str: The chatbot's response chunk by chunk.\n",
    "    \"\"\"\n",
    "\n",
    "    # context retrieval\n",
    "    text_results, image_results = context_retrieval(message[\"text\"], text_embeddings, image_embeddings, text_content_list, image_content_list)\n",
    "\n",
    "    # construct prompt\n",
    "    prompt = construct_prompt(message[\"text\"], text_results, image_results)\n",
    "    \n",
    "    # Append the user message to the conversation history\n",
    "    history.append({\"role\": \"user\", \"content\": prompt, \"images\": [image[\"image_path\"] for image in image_results]})\n",
    "    \n",
    "    # Initialize streaming from Ollama\n",
    "    stream = ollama.chat(\n",
    "        model='llama3.2-vision',\n",
    "        messages=history,  # Full chat history including the current user message\n",
    "        stream=True,\n",
    "    )\n",
    "    \n",
    "    response_text = \"\"\n",
    "    for chunk in stream:\n",
    "        content = chunk['message']['content']\n",
    "        response_text += content\n",
    "        yield response_text  # Send the response incrementally to the UI\n",
    "\n",
    "    # Append the assistant's full response to the history\n",
    "    history.append({\"role\": \"assistant\", \"content\": response_text})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ac41fc13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a Gradio ChatInterface\n",
    "gr.ChatInterface(\n",
    "    fn=stream_chat,  # The function handling the chat\n",
    "    type=\"messages\",  # Using \"messages\" to enable chat-style conversation\n",
    "    examples=[{\"text\": \"What is CLIP's contrastive loss function?\"}, \n",
    "              {\"text\": \"What are the three paths described for making LLMs multimodal?\"},\n",
    "              {\"text\": \"What is an intuitive explanation of multimodal embeddings?\"}],  # Example inputs\n",
    "    multimodal=True,\n",
    ").launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de831209",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
