# Multimodal Chatbot with CLIP and Ollama Integration

This repository contains two main components:

1. **HTML Content Parsing and CLIP Embedding Extraction**: A Python script to parse HTML files, extract structured data (including images and captions), and compute embeddings using OpenAI's CLIP model for image-text matching and multimedia search.
   
2. **Multimodal Chatbot with Ollama Integration**: A multimodal chatbot that uses text and image embeddings to provide context-based responses to user queries, leveraging the LLaMA model via Ollama API.

---

## Features

### HTML Content Parsing and CLIP Embedding Extraction
- **HTML Parsing**: Extracts text content and images from HTML files, handling article titles, sections, paragraphs, and figure captions.
- **Image Downloading**: Downloads and saves images locally from HTML `img` tags.
- **CLIP Model Integration**: Uses the CLIP model from Hugging Face to compute embeddings for both images and text.
- **Embeddings Storage**: Saves both text and image embeddings as PyTorch tensors for later use in various applications.

### Multimodal Chatbot with Ollama Integration
- **Text & Image Embeddings**: Retrieves relevant text and image content based on embeddings, offering a rich context for answering user queries.
- **Context Retrieval**: Performs similarity searches to find matching content using embeddings and returns them as context for response generation.
- **Ollama LLaMA Integration**: Uses the Ollama API and the LLaMA-3.2 Vision model for generating multimodal responses.
- **Gradio UI**: Provides an interactive web-based interface for multimodal user interaction.

---

## Requirements

```bash
pip install -r requirements.txt
```
---

## File Structure

```
/raw                    # Directory containing raw HTML files
/images                 # Directory for storing downloaded images
/data                   # Directory for saving processed data (JSON files and embeddings)
    ├── text_content.json
    ├── image_content.json
    ├── text_embeddings.pt
    └── image_embeddings.pt
parse_html.py           # Python script to parse HTML and compute CLIP embeddings
utility_functions.py    # Helper functions for saving data to JSON
main.py                 # Main file that runs the multimodal chatbot
requirements.txt        # List of dependencies
```

---

## How to Use

### Part 1: HTML Content Parsing and CLIP Embedding Extraction

1. **Prepare HTML Files**: Place your raw HTML files in the `/raw` directory. The script will process all HTML files in this directory.
2. **Run the Script**: Execute all the cells in the data_prep.ipynb file to save the extracted embeddings and json.
3. **Outputs**:
   - Text and image content are extracted and saved as JSON files in the `/data` directory.
   - CLIP model embeddings for both text and images are saved as `.pt` files for further processing.

### Part 2: Multimodal Chatbot with Ollama Integration

1. **Clone the Repository**:
   ```bash
   git clone git@github.com:varun7778/MMRAG.git
   cd MMRAG
   ```

2. **Install the Required Packages**:
   ```bash
   pip install -r requirements.txt
   ```

3. **Ensure Access to Ollama**: Download Ollama and "llama3.2-vision" model to local.

4. **Run the Chatbot**: Execute all the cells in the mmrag.ipynb file to save the extracted embeddings and json.

   This will launch a Gradio interface where you can query the model and receive multimodal responses.

---

## Detailed Explanation

### HTML Content Parsing

The script uses **BeautifulSoup** to parse HTML and extract:
- **Text**: Extracts text from `h1`, `h2`, `h3`, `p`, `ul`, `ol`, and other relevant tags.
- **Images**: Downloads images from the `img` tags and saves them locally in the `/images` directory.
- **Captions**: Extracts captions from `alt` attributes or `figcaption` tags.

### CLIP Embedding Generation

1. **Text and Image Preprocessing**:
   - Text content is tokenized and preprocessed.
   - Images are loaded and resized to the appropriate format for CLIP.

2. **Embedding Computation**:
   - **Text Embeddings**: Generated using the CLIP model to encode text.
   - **Image Embeddings**: Generated by feeding the processed images into the CLIP model.

3. **Data Saving**:
   - Text and image content are saved as JSON files.
   - Embeddings are saved as PyTorch tensors for easy integration into downstream tasks.

### Multimodal Chatbot Operation

1. **Loading Content and Embeddings**: The chatbot loads precomputed text and image embeddings from disk.
2. **User Query Processing**: When a user submits a query, the text is embedded using CLIP, and the system searches for the most relevant content using similarity measures.
3. **Context Construction**: The top results from the similarity search (text and image) are used to generate a prompt for the LLaMA model.
4. **Response Generation**: The prompt is sent to the Ollama API, and the LLaMA model generates a response, which is streamed to the user.
5. **Interactive Chat**: The Gradio interface provides a conversational interface for users to interact with the model.
