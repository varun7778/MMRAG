<!DOCTYPE html><html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><title>Multimodal Embeddings: An Introduction</title><style>
  * {
    font-family: Georgia, Cambria, "Times New Roman", Times, serif;
  }
  html, body {
    margin: 0;
    padding: 0;
  }
  h1 {
    font-size: 50px;
    margin-bottom: 17px;
    color: #333;
  }
  h2 {
    font-size: 24px;
    line-height: 1.6;
    margin: 30px 0 0 0;
    margin-bottom: 18px;
    margin-top: 33px;
    color: #333;
  }
  h3 {
    font-size: 30px;
    margin: 10px 0 20px 0;
    color: #333;
  }
  header {
    width: 640px;
    margin: auto;
  }
  section {
    width: 640px;
    margin: auto;
  }
  section p {
    margin-bottom: 27px;
    font-size: 20px;
    line-height: 1.6;
    color: #333;
  }
  section img {
    max-width: 640px;
  }
  footer {
    padding: 0 20px;
    margin: 50px 0;
    text-align: center;
    font-size: 12px;
  }
  .aspectRatioPlaceholder {
    max-width: auto !important;
    max-height: auto !important;
  }
  .aspectRatioPlaceholder-fill {
    padding-bottom: 0 !important;
  }
  header,
  section[data-field=subtitle],
  section[data-field=description] {
    display: none;
  }
  </style></head><body><article class="h-entry">
<header>
<h1 class="p-name">Multimodal Embeddings: An Introduction</h1>
</header>
<section data-field="subtitle" class="p-summary">
Mapping text and images into a common space
</section>
<section data-field="body" class="e-content">
<section name="798a" class="section section--body section--first"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h3 name="7c71" id="7c71" class="graf graf--h3 graf--leading graf--title"><strong class="markup--strong markup--h3-strong">Multimodal Embeddings: An Introduction</strong></h3><h4 name="341a" id="341a" class="graf graf--h4 graf-after--h3 graf--subtitle">Mapping text and images into a common space</h4><p name="9161" id="9161" class="graf graf--p graf-after--h4">This is the 2nd article in a <a href="https://shawhin.medium.com/list/multimodal-ai-fe9521d0e77a" data-href="https://shawhin.medium.com/list/multimodal-ai-fe9521d0e77a" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">larger series</a> on multimodal AI. In the <a href="https://towardsdatascience.com/multimodal-models-llms-that-can-see-and-hear-5c6737c981d3" data-href="https://towardsdatascience.com/multimodal-models-llms-that-can-see-and-hear-5c6737c981d3" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">previous post</a>, we saw how to augment <a href="https://shawhin.medium.com/list/large-language-models-llms-8e009ae3054c" data-href="https://shawhin.medium.com/list/large-language-models-llms-8e009ae3054c" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">large language models (LLMs)</a> to understand new data modalities (e.g., images, audio, video). One such approach relied on encoders that generate vector representations (i.e. embeddings) of non-text data. In this article, I will discuss <em class="markup--em markup--p-em">multimodal</em> embeddings and share what they can do via two practical use cases.</p><figure name="13cb" id="13cb" class="graf graf--figure graf-after--p graf--trailing"><img class="graf-image" data-image-id="1*a6BF-kEeo8rd7OW2a3JYGA.png" data-width="1280" data-height="720" data-is-featured="true" src="https://cdn-images-1.medium.com/max/800/1*a6BF-kEeo8rd7OW2a3JYGA.png"><figcaption class="imageCaption">Image from Canva.</figcaption></figure></div></div></section><section name="dd15" class="section section--body"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><figure name="ccfa" id="ccfa" class="graf graf--figure graf--iframe graf--leading"><iframe src="https://www.youtube.com/embed/YOvxh_ma5qE?feature=oembed" width="700" height="393" frameborder="0" scrolling="no"></iframe></figure><p name="88f3" id="88f3" class="graf graf--p graf-after--figure">AI research is traditionally split into distinct fields: NLP, computer vision (CV), robotics, human-computer interface (HCI), etc. However, countless practical tasks require the <strong class="markup--strong markup--p-strong">integration of these different research areas</strong> e.g. autonomous vehicles (CV + robotics), AI agents (NLP + CV + HCI), personalized learning (NLP + HCI), etc.</p><p name="e928" id="e928" class="graf graf--p graf-after--p">Although these fields aim to solve different problems and work with different data types, they all share a fundamental process. Namely, <strong class="markup--strong markup--p-strong">generating useful numerical representations of real-world phenomena</strong>.</p><p name="7db0" id="7db0" class="graf graf--p graf-after--p">Historically, this was done by hand. This means that researchers and practitioners would use their (or other people’s) expertise to explicitly transform data into a more helpful form. Today, however, <em class="markup--em markup--p-em">these can be derived another way</em>.</p><h3 name="ab69" id="ab69" class="graf graf--h3 graf-after--p"><strong class="markup--strong markup--h3-strong">Embeddings</strong></h3><p name="a08c" id="a08c" class="graf graf--p graf-after--h3"><strong class="markup--strong markup--p-strong">Embeddings</strong> are <strong class="markup--strong markup--p-strong">(useful) numerical representations of data learned implicitly through model training</strong>. For example, through learning how to predict text, BERT learned representations of text, which are helpful for many NLP tasks [1]. Another example is the Vision Transformer (ViT), trained for image classification on Image Net, which can be repurposed for other applications [2].</p><p name="9163" id="9163" class="graf graf--p graf-after--p">A key point here is that these learned embedding spaces will have some underlying structure so that <strong class="markup--strong markup--p-strong">similar concepts are located close together</strong>. As shown in the toy examples below.</p><figure name="42e7" id="42e7" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*jpmC6Kx7DxVeikEr15vooA.png" data-width="1167" data-height="641" src="https://cdn-images-1.medium.com/max/800/1*jpmC6Kx7DxVeikEr15vooA.png"><figcaption class="imageCaption">Toy represetation of text and image embeddings, respectively. Image by author.</figcaption></figure><p name="619b" id="619b" class="graf graf--p graf-after--figure">One <strong class="markup--strong markup--p-strong">key limitation</strong> of the previously mentioned models is they are restricted to a single data modality, e.g., text or images. Preventing cross-modal applications like image captioning, content moderation, image search, and more. <em class="markup--em markup--p-em">But what if we could merge these two representations?</em></p><h3 name="83af" id="83af" class="graf graf--h3 graf-after--p"><strong class="markup--strong markup--h3-strong">Multimodal Embeddings</strong></h3><p name="efaf" id="efaf" class="graf graf--p graf-after--h3">Although text and images may look very different to us, in a neural network, these are <strong class="markup--strong markup--p-strong">represented via the same mathematical object</strong>, i.e., a vector. Therefore, in principle, text, images, or any other data modality can processed by a single model.</p><p name="5698" id="5698" class="graf graf--p graf-after--p">This fact underlies <strong class="markup--strong markup--p-strong">multimodal embeddings</strong>, which <strong class="markup--strong markup--p-strong">represent multiple data modalities in the same vector space</strong> such that similar concepts are co-located (independent of their original representations).</p><figure name="0da4" id="0da4" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*5d3HBNjNIXLy0oMIvJjxWw.png" data-width="841" data-height="616" src="https://cdn-images-1.medium.com/max/800/1*5d3HBNjNIXLy0oMIvJjxWw.png"><figcaption class="imageCaption">Toy representation of multimodal embedding space. Image by author.</figcaption></figure><p name="a5b5" id="a5b5" class="graf graf--p graf-after--figure">For example, CLIP encodes text and images into a shared embedding space [3]. A key insight from CLIP is that by aligning text and image representations, the <strong class="markup--strong markup--p-strong">model is capable of 0-shot image classification on an arbitrary set of target classes</strong> since any input text can be treated as a class label (we will see a concrete example of this later).</p><p name="7606" id="7606" class="graf graf--p graf-after--p">However, this idea is not limited to text and images. Virtually any data modalities can be aligned in this way e.g., text-audio, audio-image, text-EEG, image-tabular, and text-video. Unlocking use cases such as video captioning, advanced OCR, audio transcription, video search, and EEG-to-text [4].</p><h3 name="09fe" id="09fe" class="graf graf--h3 graf-after--p"><strong class="markup--strong markup--h3-strong">Contrastive Learning</strong></h3><p name="eb1e" id="eb1e" class="graf graf--p graf-after--h3">The standard approach to aligning disparate embedding spaces is <strong class="markup--strong markup--p-strong">contrastive learning (CL)</strong>. A key intuition of CL is to <strong class="markup--strong markup--p-strong">represent different views of the same <em class="markup--em markup--p-em">information</em> similarly</strong> [5].</p><p name="a740" id="a740" class="graf graf--p graf-after--p">This consists of learning representations that <strong class="markup--strong markup--p-strong">maximize the similarity between positive pairs</strong> and <strong class="markup--strong markup--p-strong">minimize the similarity of negative pairs</strong>. In the case of an image-text model, a positive pair might be an image with an appropriate caption, while a negative pair would be an image with an irrelevant caption (as shown below).</p><figure name="55fc" id="55fc" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*AGHBVjzwjXapJSe4aUPrjg.png" data-width="1005" data-height="546" src="https://cdn-images-1.medium.com/max/800/1*AGHBVjzwjXapJSe4aUPrjg.png"><figcaption class="imageCaption">Example positive and negative pairs used in contrastive training. Image by author.</figcaption></figure><p name="6867" id="6867" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">Two key aspects</strong> <strong class="markup--strong markup--p-strong">of CL</strong> contribute to its effectiveness</p><ol class="postList"><li name="b9e9" id="b9e9" class="graf graf--li graf-after--p">Since positive and negative pairs can be curated from the data’s inherent structure (e.g., metadata from web images), CL training data <strong class="markup--strong markup--li-strong">do not require manual labeling</strong>, which unlocks larger-scale training and more powerful representations [3].</li><li name="055c" id="055c" class="graf graf--li graf-after--li">It simultaneously maximizes positive and minimizes negative pair similarity via a special loss function, as demonstrated by CLIP [3].</li></ol><figure name="c9c5" id="c9c5" class="graf graf--figure graf-after--li"><img class="graf-image" data-image-id="1*2X1aT8fzFsgbqn23zXmmAA.png" data-width="1030" data-height="545" src="https://cdn-images-1.medium.com/max/800/1*2X1aT8fzFsgbqn23zXmmAA.png"><figcaption class="imageCaption">CLIP’s contrastive loss for text-image representation alignment [3]. Image by author.</figcaption></figure><h3 name="01c5" id="01c5" class="graf graf--h3 graf-after--figure"><strong class="markup--strong markup--h3-strong">Example Code: </strong>Using CLIP for 0-shot classification and image search</h3><p name="0f0f" id="0f0f" class="graf graf--p graf-after--h3">With a high-level understanding of how multimodal embeddings work, let’s see two concrete examples of what they can do. Here, I will use the open-source <a href="https://huggingface.co/openai/clip-vit-base-patch16" data-href="https://huggingface.co/openai/clip-vit-base-patch16" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">CLIP model</a> to perform two tasks: 0-shot image classification and image search.</p><p name="66e1" id="66e1" class="graf graf--p graf-after--p graf--trailing">The <strong class="markup--strong markup--p-strong">code for these examples</strong> is freely available on the <a href="https://github.com/ShawhinT/YouTube-Blog/tree/main/multimodal-ai/2-mm-embeddings" data-href="https://github.com/ShawhinT/YouTube-Blog/tree/main/multimodal-ai/2-mm-embeddings" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">GitHub repository</a>.</p></div></div></section><section name="918f" class="section section--body"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h4 name="b6a2" id="b6a2" class="graf graf--h4 graf--leading">Use case 1: 0-shot Image Classification</h4><p name="a731" id="a731" class="graf graf--p graf-after--h4">The basic idea behind using CLIP for 0-shot image classification is to pass an image into the model along with a set of possible class labels. Then, a classification can be made by <strong class="markup--strong markup--p-strong">evaluating which text input is most similar to the input image</strong>.</p><p name="58ad" id="58ad" class="graf graf--p graf-after--p">We’ll start by importing the <a href="https://huggingface.co/docs/transformers/en/installation" data-href="https://huggingface.co/docs/transformers/en/installation" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Hugging Face Transformers library</a> so that the CLIP model can be downloaded locally. Additionally, the PIL library is used to load images in Python.</p><pre data-code-block-mode="2" spellcheck="false" data-code-block-lang="python" name="b43b" id="b43b" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content"><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> CLIPProcessor, CLIPModel<br /><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image</span></pre><p name="23c0" id="23c0" class="graf graf--p graf-after--pre">Next, we can import a version of the clip model and its associated data processor. <em class="markup--em markup--p-em">Note: the processor handles tokenizing input text and image preparation.</em></p><pre data-code-block-mode="1" spellcheck="false" data-code-block-lang="ini" name="8883" id="8883" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content"><span class="hljs-comment"># import model</span><br /><span class="hljs-attr">model</span> = CLIPModel.from_pretrained(<span class="hljs-string">&quot;openai/clip-vit-base-patch16&quot;</span>)<br /><br /><span class="hljs-comment"># import processor (handles text tokenization and image preprocessing)</span><br /><span class="hljs-attr">processor</span> = CLIPProcessor.from_pretrained(<span class="hljs-string">&quot;openai/clip-vit-base-patch16&quot;</span>) </span></pre><p name="3444" id="3444" class="graf graf--p graf-after--pre">We load in the below image of a cat and create a list of two possible class labels: “<em class="markup--em markup--p-em">a photo of a cat</em>” or “<em class="markup--em markup--p-em">a photo of a dog</em>”.</p><pre data-code-block-mode="1" spellcheck="false" data-code-block-lang="ini" name="b2ca" id="b2ca" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content"><span class="hljs-comment"># load image</span><br /><span class="hljs-attr">image</span> = Image.open(<span class="hljs-string">&quot;images/cat_cute.png&quot;</span>)<br /><br /><span class="hljs-comment"># define text classes</span><br /><span class="hljs-attr">text_classes</span> = [<span class="hljs-string">&quot;a photo of a cat&quot;</span>, <span class="hljs-string">&quot;a photo of a dog&quot;</span>]</span></pre><figure name="6fa1" id="6fa1" class="graf graf--figure graf-after--pre"><img class="graf-image" data-image-id="1*Nzo536sqahqm1Q24Ms2vmA.png" data-width="366" data-height="342" src="https://cdn-images-1.medium.com/max/800/1*Nzo536sqahqm1Q24Ms2vmA.png"><figcaption class="imageCaption">Input cat photo. Image from Canva.</figcaption></figure><p name="462a" id="462a" class="graf graf--p graf-after--figure">Next, we’ll preprocess the image/text inputs and pass them into the model.</p><pre data-code-block-mode="1" spellcheck="false" data-code-block-lang="ini" name="49b6" id="49b6" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content"><span class="hljs-comment"># pass image and text classes to processor</span><br /><span class="hljs-attr">inputs</span> = processor(text=text_classes, images=image, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>, <br />                                                    <span class="hljs-attr">padding</span>=<span class="hljs-literal">True</span>)<br /><br /><span class="hljs-comment"># pass inputs to CLIP</span><br /><span class="hljs-attr">outputs</span> = model(**inputs) <span class="hljs-comment"># <span class="hljs-doctag">note:</span> &quot;**&quot; unpacks dictionary items</span></span></pre><p name="9aee" id="9aee" class="graf graf--p graf-after--pre">To make a class prediction, we must extract the image logits and evaluate which class corresponds to the maximum.</p><pre data-code-block-mode="1" spellcheck="false" data-code-block-lang="makefile" name="df2b" id="df2b" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content"><span class="hljs-comment"># image-text similarity score</span><br />logits_per_image = outputs.logits_per_image <br /><span class="hljs-comment"># convert scores to probs via softmax</span><br />probs = logits_per_image.softmax(dim=1) <br /><br /><span class="hljs-comment"># print prediction</span><br />predicted_class = text_classes[probs.argmax()]<br />print(predicted_class, <span class="hljs-string">&quot;| Probability = &quot;</span>, <br />                       round(float(probs[0][probs.argmax()]),4))</span></pre><pre data-code-block-mode="0" spellcheck="false" name="a9da" id="a9da" class="graf graf--pre graf-after--pre graf--preV2"><span class="pre--content">&gt;&gt; a photo of a cat | Probability =  0.9979</span></pre><p name="2e22" id="2e22" class="graf graf--p graf-after--pre">The model nailed it with a 99.79% probability that it’s a cat photo. However, this was a super easy one. Let’s see what happens when we change the class labels to: “<em class="markup--em markup--p-em">ugly cat</em>” and “<em class="markup--em markup--p-em">cute cat</em>” for the same image.</p><pre data-code-block-mode="0" spellcheck="false" name="9246" id="9246" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content">&gt;&gt; cute cat | Probability =  0.9703</span></pre><p name="d97a" id="d97a" class="graf graf--p graf-after--pre">The model easily identified that the image was indeed a cute cat. Let’s do something more challenging like the labels: “<em class="markup--em markup--p-em">cat meme</em>” or “<em class="markup--em markup--p-em">not cat meme</em>”.</p><pre data-code-block-mode="0" spellcheck="false" name="8824" id="8824" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content">&gt;&gt; not cat meme | Probability =  0.5464</span></pre><p name="0a64" id="0a64" class="graf graf--p graf-after--pre">While the model is less confident about this prediction with a 54.64% probability, it correctly implies that the image is not a meme.</p><h4 name="ac9a" id="ac9a" class="graf graf--h4 graf-after--p">Use case 2: Image Search</h4><p name="223c" id="223c" class="graf graf--p graf-after--h4">Another application of CLIP is essentially the inverse of Use Case 1. Rather than identifying which text label matches an input image, we can evaluate <strong class="markup--strong markup--p-strong">which image (in a set) best matches a text input (i.e. query)</strong>—in other words, performing a search over images.</p><p name="5e55" id="5e55" class="graf graf--p graf-after--p">We start by storing a set of images in a list. Here, I have three images of a cat, dog, and goat, respectively.</p><pre data-code-block-mode="2" spellcheck="false" data-code-block-lang="python" name="4749" id="4749" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content"><span class="hljs-comment"># create list of images to search over</span><br />image_name_list = [<span class="hljs-string">&quot;images/cat_cute.png&quot;</span>, <span class="hljs-string">&quot;images/dog.png&quot;</span>, <span class="hljs-string">&quot;images/goat.png&quot;</span>]<br /><br />image_list = []<br /><span class="hljs-keyword">for</span> image_name <span class="hljs-keyword">in</span> image_name_list:<br />    image_list.append(Image.<span class="hljs-built_in">open</span>(image_name))</span></pre><p name="f6ea" id="f6ea" class="graf graf--p graf-after--pre">Next, we can define a query like “<em class="markup--em markup--p-em">a cute dog</em>” and pass it and the images into CLIP.</p><pre data-code-block-mode="2" spellcheck="false" data-code-block-lang="python" name="9187" id="9187" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content"><span class="hljs-comment"># define a query</span><br />query = <span class="hljs-string">&quot;a cute dog&quot;</span><br /><br /><span class="hljs-comment"># pass images and query to CLIP</span><br />inputs = processor(text=query, images=image_list, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>, <br />                                                  padding=<span class="hljs-literal">True</span>)</span></pre><p name="9bcc" id="9bcc" class="graf graf--p graf-after--pre">We can then match the best image to the input text by extracting the text logits and evaluating the image corresponding to the maximum.</p><pre data-code-block-mode="2" spellcheck="false" data-code-block-lang="python" name="0f86" id="0f86" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content"><span class="hljs-comment"># compute logits and probabilities</span><br />outputs = model(**inputs)<br />logits_per_text = outputs.logits_per_text<br />probs = logits_per_text.softmax(dim=<span class="hljs-number">1</span>)<br /><br /><span class="hljs-comment"># print best match</span><br />best_match = image_list[probs.argmax()]<br />prob_match = <span class="hljs-built_in">round</span>(<span class="hljs-built_in">float</span>(probs[<span class="hljs-number">0</span>][probs.argmax()]),<span class="hljs-number">4</span>)<br /><br /><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Match probability: &quot;</span>,prob_match)<br />display(best_match)</span></pre><pre data-code-block-mode="0" spellcheck="false" name="146a" id="146a" class="graf graf--pre graf-after--pre graf--preV2"><span class="pre--content">&gt;&gt; Match probability:  0.9817</span></pre><figure name="43aa" id="43aa" class="graf graf--figure graf-after--pre"><img class="graf-image" data-image-id="1*4wnqr5p_7N3QD5EkXIQeew.png" data-width="366" data-height="273" src="https://cdn-images-1.medium.com/max/800/1*4wnqr5p_7N3QD5EkXIQeew.png"><figcaption class="imageCaption">Best match for query “a cute dog”. Image from Canva.</figcaption></figure><p name="516e" id="516e" class="graf graf--p graf-after--figure">We see that (again) the model nailed this simple example. But let’s try some trickier examples.</p><pre data-code-block-mode="2" spellcheck="false" data-code-block-lang="python" name="c49e" id="c49e" class="graf graf--pre graf-after--p graf--preV2"><span class="pre--content">query = <span class="hljs-string">&quot;something cute but metal 🤘&quot;</span></span></pre><pre data-code-block-mode="0" spellcheck="false" name="3bbd" id="3bbd" class="graf graf--pre graf-after--pre graf--preV2"><span class="pre--content">&gt;&gt; Match probability:  0.7715</span></pre><figure name="ce2b" id="ce2b" class="graf graf--figure graf-after--pre"><img class="graf-image" data-image-id="1*tIY3_ONQQT_cracAPWm8NQ.png" data-width="288" data-height="256" src="https://cdn-images-1.medium.com/max/800/1*tIY3_ONQQT_cracAPWm8NQ.png"><figcaption class="imageCaption">Best match for query “something cute but metal 🤘”. Image from Canva.</figcaption></figure><pre data-code-block-mode="2" spellcheck="false" data-code-block-lang="python" name="fd33" id="fd33" class="graf graf--pre graf-after--figure graf--preV2"><span class="pre--content">query = <span class="hljs-string">&quot;a good boy&quot;</span></span></pre><pre data-code-block-mode="0" spellcheck="false" name="09d1" id="09d1" class="graf graf--pre graf-after--pre graf--preV2"><span class="pre--content">&gt;&gt; Match probability:  0.8248</span></pre><figure name="16c4" id="16c4" class="graf graf--figure graf-after--pre"><img class="graf-image" data-image-id="1*4wnqr5p_7N3QD5EkXIQeew.png" data-width="366" data-height="273" src="https://cdn-images-1.medium.com/max/800/1*4wnqr5p_7N3QD5EkXIQeew.png"><figcaption class="imageCaption">Best match for query “a good boy”. Image from Canva.</figcaption></figure><pre data-code-block-mode="2" spellcheck="false" data-code-block-lang="python" name="5b79" id="5b79" class="graf graf--pre graf-after--figure graf--preV2"><span class="pre--content">query = <span class="hljs-string">&quot;the best pet in the world&quot;</span></span></pre><pre data-code-block-mode="0" spellcheck="false" name="6fda" id="6fda" class="graf graf--pre graf-after--pre graf--preV2"><span class="pre--content">&gt;&gt; Match probability:  0.5664</span></pre><figure name="8d8d" id="8d8d" class="graf graf--figure graf-after--pre"><img class="graf-image" data-image-id="1*Nzo536sqahqm1Q24Ms2vmA.png" data-width="366" data-height="342" src="https://cdn-images-1.medium.com/max/800/1*Nzo536sqahqm1Q24Ms2vmA.png"><figcaption class="imageCaption">Best match for query “the best pet in the world”. Image from Canva.</figcaption></figure><p name="4918" id="4918" class="graf graf--p graf-after--figure">Although this last prediction is quite controversial, all the other matches were spot on! This is likely since images like these are ubiquitous on the internet and thus were seen many times in CLIP’s pre-training.</p><div name="9ce6" id="9ce6" class="graf graf--mixtapeEmbed graf-after--p"><a href="https://github.com/ShawhinT/YouTube-Blog/tree/main/multimodal-ai/2-mm-embeddings" data-href="https://github.com/ShawhinT/YouTube-Blog/tree/main/multimodal-ai/2-mm-embeddings" class="markup--anchor markup--mixtapeEmbed-anchor" title="https://github.com/ShawhinT/YouTube-Blog/tree/main/multimodal-ai/2-mm-embeddings"><strong class="markup--strong markup--mixtapeEmbed-strong">YouTube-Blog/multimodal-ai/2-mm-embeddings at main · ShawhinT/YouTube-Blog</strong><br><em class="markup--em markup--mixtapeEmbed-em">Codes to complement YouTube videos and blog posts on Medium. - YouTube-Blog/multimodal-ai/2-mm-embeddings at main ·…</em>github.com</a><a href="https://github.com/ShawhinT/YouTube-Blog/tree/main/multimodal-ai/2-mm-embeddings" class="js-mixtapeImage mixtapeImage u-ignoreBlock" data-media-id="e9efe45296ce4a92c75777d69a05524a" data-thumbnail-img-id="0*9DsPlXG4I8OrEOUw" style="background-image: url(https://cdn-images-1.medium.com/fit/c/160/160/0*9DsPlXG4I8OrEOUw);"></a></div><h3 name="7f87" id="7f87" class="graf graf--h3 graf-after--mixtapeEmbed">What’s Next?</h3><p name="e174" id="e174" class="graf graf--p graf-after--h3">Multimodal embeddings unlock countless AI use cases that involve multiple data modalities. Here, we saw two such use cases, i.e., 0-shot image classification and image search using CLIP.</p><p name="d1c5" id="d1c5" class="graf graf--p graf-after--p">Another practical application of models like CLIP is multimodal RAG, which consists of the automated retrieval of multimodal context to an LLM. In the next article of this <a href="https://shawhin.medium.com/list/multimodal-ai-fe9521d0e77a" data-href="https://shawhin.medium.com/list/multimodal-ai-fe9521d0e77a" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">series</a>, we will see how this works under the hood and review a concrete example.</p><p name="d03b" id="d03b" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">More on Multimodal models 👇</strong></p><div name="9925" id="9925" class="graf graf--mixtapeEmbed graf-after--p graf--trailing"><a href="https://shawhin.medium.com/list/fe9521d0e77a" data-href="https://shawhin.medium.com/list/fe9521d0e77a" class="markup--anchor markup--mixtapeEmbed-anchor" title="https://shawhin.medium.com/list/fe9521d0e77a"><strong class="markup--strong markup--mixtapeEmbed-strong">Multimodal AI</strong><br><em class="markup--em markup--mixtapeEmbed-em">Edit description</em>shawhin.medium.com</a><a href="https://shawhin.medium.com/list/fe9521d0e77a" class="js-mixtapeImage mixtapeImage mixtapeImage--mediumCatalog  u-ignoreBlock" data-media-id="6105bb036ca97dea6117e7ac234d7d0a" data-thumbnail-img-id="0*d9f0f35363d134d5ddb508e140d32649a2f1ca00.jpeg" style="background-image: url(https://cdn-images-1.medium.com/fit/c/304/160/0*d9f0f35363d134d5ddb508e140d32649a2f1ca00.jpeg);"></a></div></div></div></section><section name="a479" class="section section--body section--last"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><p name="7985" id="7985" class="graf graf--p graf--leading"><strong class="markup--strong markup--p-strong">My website</strong>: <a href="https://www.shawhintalebi.com/" data-href="https://www.shawhintalebi.com/" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">https://www.shawhintalebi.com/</a></p><ul class="postList"><li name="3c3f" id="3c3f" class="graf graf--li graf-after--p">[1] <a href="https://arxiv.org/abs/1810.04805" data-href="https://arxiv.org/abs/1810.04805" class="markup--anchor markup--li-anchor" rel="noopener" target="_blank">BERT</a></li><li name="6bd0" id="6bd0" class="graf graf--li graf-after--li">[2] <a href="https://arxiv.org/abs/2010.11929" data-href="https://arxiv.org/abs/2010.11929" class="markup--anchor markup--li-anchor" rel="noopener" target="_blank">ViT</a></li><li name="df6f" id="df6f" class="graf graf--li graf-after--li">[3] <a href="https://arxiv.org/abs/2103.00020" data-href="https://arxiv.org/abs/2103.00020" class="markup--anchor markup--li-anchor" rel="noopener" target="_blank">CLIP</a></li><li name="c395" id="c395" class="graf graf--li graf-after--li">[4] <a href="https://arxiv.org/abs/2410.07507" data-href="https://arxiv.org/abs/2410.07507" class="markup--anchor markup--li-anchor" rel="noopener" target="_blank">Thought2Text: Text Generation from EEG Signal using Large Language Models (LLMs)</a></li><li name="0cd0" id="0cd0" class="graf graf--li graf-after--li graf--trailing">[5] <a href="https://arxiv.org/abs/2002.05709" data-href="https://arxiv.org/abs/2002.05709" class="markup--anchor markup--li-anchor" rel="noopener" target="_blank">A Simple Framework for Contrastive Learning of Visual Representations</a></li></ul></div></div></section>
</section>
<footer><p>By <a href="https://medium.com/@shawhin" class="p-author h-card">Shaw Talebi</a> on <a href="https://medium.com/p/5dc36975966f"><time class="dt-published" datetime="2024-11-29T15:02:12.635Z">November 29, 2024</time></a>.</p><p><a href="https://medium.com/@shawhin/multimodal-embeddings-an-introduction-5dc36975966f" class="p-canonical">Canonical link</a></p><p>Exported from <a href="https://medium.com">Medium</a> on December 2, 2024.</p></footer></article></body></html>